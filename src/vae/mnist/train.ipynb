{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# model\n",
    "from mnist_vae import MnistVAE\n",
    "\n",
    "# load loss functions\n",
    "import sys\n",
    "sys.path.append('./../../loss')\n",
    "from loss_provider import LossProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './../../datasets/MNIST/'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "data_dim = (1,32,32)\n",
    "data_size = np.prod(data_dim)\n",
    "batch_size = 128\n",
    "allowed_labels = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_set(dataset, allowed_labels):\n",
    "    to_drop = [i for i in range(len(dataset)) if dataset[i][1] in allowed_labels]\n",
    "    return torch.utils.data.Subset(dataset, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key word args for loading data\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == \"cuda\" else {}\n",
    "\n",
    "# transformers\n",
    "transformers = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),         # grayscale images\n",
    "    transforms.Pad((data_dim[1] - 28) // 2, fill=0, padding_mode='constant'), # padding\n",
    "    transforms.ToTensor()                                # as tensors\n",
    "])\n",
    "\n",
    "train_set = datasets.ImageFolder(dataset + 'train/', transform=transformers)\n",
    "test_set = datasets.ImageFolder(dataset + 'test/', transform=transformers)\n",
    "\n",
    "# filter labels\n",
    "train_set = filter_set(train_set, allowed_labels)\n",
    "test_set = filter_set(test_set, allowed_labels)\n",
    "\n",
    "# load datasets and make them easily fetchable in DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "\n",
    "# load datasets and make them easily fetchable in DataLoaders\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "to_img = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction samples:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=274x70 at 0x7F0459636358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.random.randint(0,len(test_set),16)\n",
    "sample_set = [test_set[i][0].numpy() for i in idx]\n",
    "sample_set = torch.tensor(sample_set, dtype=torch.float32)\n",
    "sample_set = sample_set.to(device)\n",
    "\n",
    "grid = torchvision.utils.make_grid(sample_set.cpu(), nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "print('reconstruction samples:')\n",
    "to_img(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reconstruction(model, logger):\n",
    "    # plots reconstruction of a couple inputs\n",
    "    with torch.no_grad():\n",
    "        z, _ = model.encode(sample_set)\n",
    "        recon = model.decode(z)\n",
    "    comparison = torch.cat([sample_set[:8], recon[:8],sample_set[8:], recon[8:]])\n",
    "    torchvision.utils.save_image(comparison.cpu(),\n",
    "             logger.path + 'reconstruction_' + str(logger.minibatch) + '.png', nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpolation samples:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGgAAAAkCAIAAADq5OjhAAAEnklEQVR4nO2XS0hyTRjHJ0ujC/VmQSRR0Y1SECqhCOyyKqmMIKhFKzdFRSCYQUS1qAi6kJukRQVdFiJGnFpLN7pACBp2A0GCMPAUnO7omfFdHPD1q+/1On7Bx/ntzsxz/v+Z58yZeQYAFhYWFhYWFpYfJeanB/AzqFSqhIQEsVjc3t4OANBqtScnJ+vr69Hyk0qlsbGxQqFQrVbv7++r1Wq1Wi2VSqPlFx10Oh38xu3tbU5ODn6zlJSUnZ2dt7c3kiSfn5+RD29vb06nk/l0WNBoNB6PR6FQ4BL0xTdrVqt1fn5+e3ubeRweHsbvp9VqvZmyWq17e3sEQRAEsbu7yzRSFCUWiyM3kslkj4+PEMKdnZ3I1b4gkUhcLheE0GKx5OXlJScnAwB4PJ7JZIIQzs7OYvYTiUROpxMhdHd3V1dXl52dzVgCADgczvj4OE3THo/HYDCkpaVF6NXY2EiSZJQS19LS4na7LRZLVlaWt3F4ePjz8xNCWF9fj9mvqqoKIQQh7Ovr+9eAqakpl8uFEGpqaorcrqamBkJos9lKS0sjV/tCbm4un8/3bTGbzcyvij9xtbW1CKGVlRU/MTabDSG0vLyMxRFCSNN0Q0MDFjU/DA4Ofnx8QAiPj48TExMxqx8cHCCEuru7/cQsLi4y2x8WRyZxh4eHWNT+RnNzM5M1h8NRW1sb0rucgBH5+fkCgYCiqIuLCz9hRqMxJONgEIvFcrkcu6wXiUTC4/EAADqdbn9/H7P66OgoQkiv1/sPa29vx77iaJru7+/HIvid7e3t9/d3COHq6qr3rAuewCuus7OToiiNRhPW8MJEqVTGxMTExETrYpOVlVVdXR0fH0+S5MTExOvra6gKccEEXV9fHx0dhT688PF4PB6PJ3r6BoMhPT0dALCxsWGz2cJQCLDikpKSuFxuOEOLDIIgHA4HACAjIwO7uFwuLy8vBwDs7e2NjY1h1wcAAIVCgRA6OTkJGLm2toYQMpvNuKxvbm6YbQ6XIEN6evrp6SlTuM3NzeEV/0OQiauoqHh6esJVADPIZDJmeqEWCv6ZmppiZA0GQxhnQrAEk7iKiorNzU2E0OHhYVxcUJtmMMhkMmbFLSws4NIEADC3Kwih760rDALM0263v7y8+AmIjY1VqVQdHR339/cqlQr7nxU9+Hy+2+32PlIU5Xa7uVxuamoqACAtLU2pVDJdEMKhoaH393ff1wMkzmg03t/fp6SkZGRkkCTp2yUWi3t7e8vLyyUSCQCgq6vr7OwMy5T+GywWi++jXq93OByZmZkdHR3fgx8eHiYnJ31bAhdKl5eXJSUlJpPp4eHBt72yspI50UmSJAhiYGDgyzeJEKFQuLu7m5uby+FwCgsLwysavrO1tdXa2uo/hqZphBAAgCCI8/NzAMDR0dGX/Spw4tra2kZGRsrKyr53IYSenp7m5+enp6dDGHvQzMzMMJVwcXExrsQBANRqtbfGEolE3iW2srJit9sBAFtbW1dXVxicBAKBxWJB/2RpaamnpweD+t8pKiqiaRpCWFBQEFWj/xu/fv0yGo39/f3Mhs3CwsLCwsLCwvKH30sggotv3/wRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=104x36 at 0x7F0450E40080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpol_set = []\n",
    "interpol_labels = []\n",
    "assert len(allowed_labels) >= 3, \"need at least 3 labels for interpolation\"\n",
    "for i in range(len(test_set)):\n",
    "    if test_set[i][1] not in interpol_labels:\n",
    "        interpol_labels.append(test_set[i][1])\n",
    "        interpol_set.append(test_set[i][0].numpy())\n",
    "    if len(interpol_labels) >= 3:\n",
    "        break\n",
    "        \n",
    "interpol_set = torch.tensor(interpol_set, dtype=torch.float32)\n",
    "interpol_set = interpol_set.to(device)\n",
    "\n",
    "grid = torchvision.utils.make_grid(interpol_set.cpu(), nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "print('interpolation samples:')\n",
    "to_img(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_interpol(model, logger):\n",
    "    # plots interpolation of a couple inputs\n",
    "    with torch.no_grad():\n",
    "        N = 16\n",
    "        zs = model.encode(interpol_set)[0].detach().cpu().numpy()\n",
    "        if zs.shape[1] == 2:\n",
    "            # 2 dimensional latent space. Interpolate over entire latent space\n",
    "            z0 = np.array([-2., 2.])\n",
    "            z0to1 = np.array([4., 0.])\n",
    "            z0to2 = np.array([0., -4.])\n",
    "        else:\n",
    "            # multi-dimensional latent space. interpolate between samples\n",
    "            z0 = zs[0]\n",
    "            z0to1 = zs[1] - z0\n",
    "            z0to2 = zs[2] - z0\n",
    "        zs = []\n",
    "        for y in range(N):\n",
    "            for x in range(N):\n",
    "                zs.append(z0 + z0to1 * (x/(N-1)) + z0to2 * (y/(N-1)))\n",
    "        zs = torch.tensor(zs, dtype=torch.float32)\n",
    "        zs = zs.to(device)\n",
    "        recon = model.decode(zs)\n",
    "    torchvision.utils.save_image(recon.cpu(),\n",
    "             logger.path + 'interpol_' + str(logger.minibatch) + '.png', nrow=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latents(model, n, epoch, logger):\n",
    "    # samples random values from latent space\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(n, model.latent_space).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        torchvision.utils.save_image(sample.view(n, data_dim[0], data_dim[1], data_dim[2]),\n",
    "                   logger.path + 'sample_' + str(logger.minibatch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(output_path):\n",
    "    model = torch.load(output_path + 'model.pckl')\n",
    "    logger = pickle.load(open(output_path + 'log.pckl', \"rb\"))\n",
    "    loss_fun = pickle.load(open(output_path + 'loss_fun.pckl', \"rb\"))\n",
    "    return logger, model, loss_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(logger, model, loss_fun):\n",
    "    torch.save(model, logger.path + 'model.pckl')\n",
    "    pickle.dump(logger, open(logger.path + 'log.pckl', \"wb\"))\n",
    "    pickle.dump(loss_fun, open(logger.path + 'loss_fun.pckl', \"wb\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.minibatch = 0\n",
    "        \n",
    "        # make output dir\n",
    "        if not os.path.isdir(self.path):\n",
    "            os.mkdir(self.path)\n",
    "        \n",
    "        stats = {}\n",
    "        for l in ['train', 'test']:\n",
    "            stats[l] = {'losses': [],\n",
    "                         'recon_losses': [],\n",
    "                         'recon_losses2': [],\n",
    "                         'kl_losses': [],\n",
    "                         'mean_z': [],\n",
    "                         'var_z': [],\n",
    "                         'var_mu': [],\n",
    "                         'idx': []}\n",
    "        self.stats = stats\n",
    "        return\n",
    "        \n",
    "    def train_step(self, kl_loss, recon_loss, recon_loss2, z, mu):\n",
    "        self.step('train', self.minibatch, kl_loss, recon_loss, recon_loss2, z, mu)\n",
    "        self.minibatch += 1\n",
    "        return\n",
    "        \n",
    "    def test_step(self, kl_loss, recon_loss, recon_loss2, z, mu):\n",
    "        return self.step('test', self.minibatch, kl_loss, recon_loss, recon_loss2, z, mu)\n",
    "    \n",
    "    def step(self, env, idx, kl_loss, recon_loss, recon_loss2, z, mu):\n",
    "        \"\"\"\n",
    "        logs one training step\n",
    "        \"\"\"\n",
    "        self.stats[env]['recon_losses'].append(recon_loss.item() / batch_size)\n",
    "        self.stats[env]['recon_losses2'].append(recon_loss2.item() / batch_size)\n",
    "        self.stats[env]['kl_losses'].append(kl_loss.item() / batch_size)\n",
    "        self.stats[env]['losses'].append(recon_loss.item() + kl_loss.item() / batch_size)\n",
    "        self.stats[env]['mean_z'].append(z.mean().item())\n",
    "        self.stats[env]['var_z'].append(z.var().item())\n",
    "        self.stats[env]['var_mu'].append(mu.var().item())\n",
    "        self.stats[env]['idx'].append(idx)\n",
    "        return\n",
    "    \n",
    "    def plot(self):\n",
    "        if 2 >= len(self.stats['train']['losses']):\n",
    "            print('need at least 2 epochs of data for plot')\n",
    "            return\n",
    "        xs_train = self.stats['train']['idx']\n",
    "        xs_test = self.stats['test']['idx']\n",
    "\n",
    "        # Two subplots, unpack the axes array immediately\n",
    "        f, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(6,8))\n",
    "        ax1.semilogy(xs_train, self.stats['train']['recon_losses'], label='train')\n",
    "        ax1.semilogy(xs_test, self.stats['test']['recon_losses'], label='test')\n",
    "        ax1.set_ylabel('$\\mathcal{L}_{rec}$')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        ax2.plot(xs_train, self.stats['train']['mean_z'])\n",
    "        ax2.plot(xs_test, self.stats['test']['mean_z'])\n",
    "        ax2.set_ylabel('$\\mu(z)$')\n",
    "        ax3.plot(xs_train, self.stats['train']['var_z'])\n",
    "        ax3.plot(xs_test, self.stats['test']['var_z'])\n",
    "        ax3.set_ylabel('$\\sigma^2(z)$')\n",
    "        ax4.plot(xs_train, np.array(self.stats['train']['var_z']) - np.array(self.stats['train']['var_mu']))\n",
    "        ax4.plot(xs_test, np.array(self.stats['test']['var_z']) - np.array(self.stats['test']['var_mu']))\n",
    "        ax4.set_xlabel('Minibatch')\n",
    "        ax4.set_ylabel('$\\sigma^2(z) - \\sigma^2(\\mu(X))$')\n",
    "        f.savefig(self.path + 'training.pdf')\n",
    "        plt.close(fig=f)\n",
    "        return\n",
    "    \n",
    "    def plot_loss_comparison(self):\n",
    "        if 2 >= len(self.stats['train']['losses']):\n",
    "            # print('need at least 2 epochs of data for plot')\n",
    "            return\n",
    "        xs_train = self.stats['train']['idx']\n",
    "        xs_test = self.stats['test']['idx']\n",
    "        f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(6,6))\n",
    "        ax1.semilogy(xs_train, self.stats['train']['recon_losses'], label='train')\n",
    "        ax1.semilogy(xs_test, self.stats['test']['recon_losses'], label='test')\n",
    "        ax1.set_ylabel('$\\mathcal{L}_{rec}$ of training objective')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        ax2.semilogy(xs_train, self.stats['train']['recon_losses2'], label='train')\n",
    "        ax2.semilogy(xs_test, self.stats['test']['recon_losses2'], label='test')\n",
    "        ax2.set_ylabel('other $\\mathcal{L}_{rec}$ metric')\n",
    "        ax2.set_xlabel('Minibatch')\n",
    "        f.savefig(self.path + 'comparison.pdf')\n",
    "        plt.close(fig=f)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_loss(mu, logvar):\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2), sum is over all dim of the latent distribution z\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch, model, train_set, loss_fun, loss_fun_comparison, lam, optimizer, logger): # train for one epoch\n",
    "    # set model to training mode, affects eg. dropout layers\n",
    "    model.train() \n",
    "    \n",
    "    for (data, _label) in train_set: # iterate over minibacthes\n",
    "        data = data.to(device) # cast data to gpu\n",
    "        optimizer.zero_grad() # set gradients to 0\n",
    "\n",
    "        recon_batch, mu, logvar, z = model(data) # run model\n",
    "        \n",
    "        # calculate loss\n",
    "        reconstruct_loss = lam * loss_fun(recon_batch, data)\n",
    "        kl_loss = regularization_loss(mu, logvar)\n",
    "        loss = reconstruct_loss + kl_loss\n",
    "        \n",
    "        if loss_fun_comparison is None:\n",
    "            reconstruct_loss2 = torch.tensor([0.])\n",
    "        else:\n",
    "            reconstruct_loss2 = lam * loss_fun_comparison(data, recon_batch)\n",
    "        \n",
    "        # propagate loss\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        \n",
    "        # log\n",
    "        logger.train_step(kl_loss, reconstruct_loss, reconstruct_loss2, z, mu)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(epoch, model, test_set, loss_fun, loss_fun_comparison, lam, logger): # train for one epoch\n",
    "    # set model to eval mode, affects eg. dropout layers\n",
    "    model.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _label in test_set: # iterate over minibacthes\n",
    "            data = data.to(device) # cast data to gpu\n",
    "            recon_batch, mu, logvar, z = model(data) # run model\n",
    "\n",
    "            # calculate loss\n",
    "            reconstruct_loss = lam * loss_fun(recon_batch, data)\n",
    "            kl_loss = regularization_loss(mu, logvar)\n",
    "            loss = reconstruct_loss + kl_loss\n",
    "            \n",
    "            if loss_fun_comparison is None:\n",
    "                reconstruct_loss2 = torch.tensor([0.])\n",
    "            else:\n",
    "                reconstruct_loss2 = lam * loss_fun_comparison(data, recon_batch)\n",
    "\n",
    "            # log\n",
    "            logger.test_step(kl_loss, reconstruct_loss, reconstruct_loss2, z, mu)\n",
    "                \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main programm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, initial_lr, lr_decline=(0.5 ** (1/100))):\n",
    "    \"\"\"declines the learning rate by a factor of $\\sqrt{100}{0.5}$ ever epoch, a reduction by half over every 100 epochs\"\"\"\n",
    "    lr = initial_lr * lr_decline**epoch\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set, test_set, loss_fun, loss_fun_comparison=None, lam=1, epochs=10, initial_lr=5e-4,  logger=Logger('./results/')):\n",
    "    model = model.to(device)\n",
    "    loss_fun = loss_fun.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    lowest_loss = 99999\n",
    "    \n",
    "    # train\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        adjust_learning_rate(optimizer, epoch, initial_lr)\n",
    "        logger.plot()\n",
    "        if not(loss_fun_comparison is None):\n",
    "            logger.plot_loss_comparison()\n",
    "\n",
    "        train_step(epoch, model, train_set, loss_fun, loss_fun_comparison, lam, optimizer, logger)\n",
    "        \n",
    "        loss = test_step(epoch, model, test_set, loss_fun, loss_fun_comparison, lam, logger)\n",
    "\n",
    "        #clear_output()\n",
    "        sample_latents(model, 64, epoch, logger)\n",
    "        sample_interpol(model, logger)\n",
    "        sample_reconstruction(model, logger)\n",
    "        \n",
    "        \n",
    "            \n",
    "        # checkpoint\n",
    "        if loss < lowest_loss:\n",
    "            lowest_loss = loss\n",
    "            save_model(logger, model, loss_fun)\n",
    "            \n",
    "    return logger, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available modes: ['LA', 'RGB']\n",
      "available loss functions: ['L1', 'L2', 'SSIM', 'Watson-dct', 'Watson-fft', 'Watson-vgg', 'Deeploss-vgg', 'Deeploss-squeeze']\n"
     ]
    }
   ],
   "source": [
    "provider = LossProvider()\n",
    "print('available modes: {}'.format(provider.color_models))\n",
    "print('available loss functions: {}'.format(provider.loss_functions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating function deeploss-squeeze with lambda e^6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f266d6dc50442818c8d0d341d19d22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need at least 2 epochs of data for plot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steffen/study/VAE-perceptual-loss/venv-perceptual-loss/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluating function deeploss-squeeze with lambda e^7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc63d6580df437abe1829dd32952343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need at least 2 epochs of data for plot\n"
     ]
    }
   ],
   "source": [
    "# trains l2 loss comparison\n",
    "for loss_name, lam in [('ssim', 9), ('watson-fft', 1), ('deeploss-vgg', 9), ('deeploss-squeeze', 9),]:\n",
    "    print('evaluating function {} with lambda e^{}'.format(loss_name, lam))\n",
    "    model = MnistVAE(latent_space=2, input_size=data_dim[1:])\n",
    "    loss_fun = provider.get_loss_function(loss_name, 'LA')\n",
    "    train_model(model=model, \n",
    "                        train_set=train_loader, \n",
    "                        test_set=test_loader, \n",
    "                        loss_fun=loss_fun,\n",
    "                        loss_fun_comparison=None,\n",
    "                        lam=np.exp(lam),\n",
    "                        epochs=250,\n",
    "                        initial_lr=1e-4,\n",
    "                        logger=Logger('./results/' + loss_name + '_lam_e'+str(lam)+'/')\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-perceptual-sim2",
   "language": "python",
   "name": "venv-perceptual-sim2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
